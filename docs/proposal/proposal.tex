\documentclass[12pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{titling}
\usepackage{titlesec}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage{multirow}

\setlength{\parskip}{1.0em}
\setlength{\headsep}{1em}
\setlength{\headheight}{0em}
\setlength{\parindent}{0em}

\titlespacing*{\section}
{0pt}{1em}{0pt}
\titlespacing*{\subsection}
{0pt}{0pt}{0pt}
\titlespacing*{\subsubsection}
{0pt}{0pt}{0pt}

\pretitle{\noindent \LARGE}
\posttitle{}
\preauthor{\hfill}
\postauthor{;\em}
\predate{}
\postdate{}

\title{Project Proposal}
\author{Andrew Palmer, Mark Roth}

\begin{document}
    \maketitle

    \section{Problem Description}
    Many applications are structured into client and server portions, separated by the Internet.
    Typically these applications use some kind of remote procedure call protocol to communicate between the client and the server.
    When you look at what many of these remote procedures actually accomplish, however, you quickly find that they are often dedicated to creating, reading, updating, and deleting resources on the server -- in effect, data synchronization.
    Often these portions of the RPC interface start to become an architectural sinkhole, where they don't really do anything meaningful, only proxying data directly to and from the database.
    As a result, they become a source of code bloat and unnecessary complexity that increases development time and makes it harder to add new features, but doesn't add any value to the system.
    The worst case of this problem is where, baring security, you could just expose the database to the clients directly and it would make little difference to the server's provided functionality.

    These fragile CRUD remote interfaces have another drawback that manifests on the client side.
    The highly specific server interface tends to create very specific interactions around different resources in the client.
    Over time this results in duplicated code as each type of resource ends up with its own ad-hoc synchronization mechanism.
    This can develop into considerable complexity, as discussed by Facebook in their \href{https://engineering.fb.com/2020/03/02/data-infrastructure/messenger/}{Project LightSpeed case study}, where among other anti-patterns, this managed to accumulate 1.7 million lines of code in the Facebook Messenger client.
    Another problem with these ad-hoc data models in the client is that they tend to be fairly inflexible, making it harder to build rich user interfaces without duplicating views or to quickly iterate on new user interface designs.
    Often in such systems adding a new user interface requires modifying the UI code, the inner client code that interfaces with the network, and adding new code to the server.

        \subsection{Desirable Architectural Characteristics}
            \subsubsection{Robustness}
            % Robustness is critical for solving data synchronization. If data is corrupted due to errors, ...
            \subsubsection{Scalability}
            % Lots of users. Not as much data/user but still significant amount.
            \subsubsection{Performance}
            % Slow sync is bad for users.
            \subsubsection{Portability}
            % Lots of kinds of clients
            % Don't want to be tied to a single cloud provider for a generic service
            \subsubsection{Extensibility}
            % Different domain applications will still have specific needs

    \section{Proposed Solution}
    % Overview
    % TODO: should we mention how this is like Facebook's solution? probably?
    We propose to create an abstract service that provides data synchronization (the ``Sync Service"), allowing us to reuse the same service to manage data synchronization for many different applications.
    The Sync Service takes care of keeping the data stored on the server and cached on the client in sync. It will also perform access control for data, making sure users only have access to their own data.
    The Sync Service will be extensible in three ways: validation, conflict resolution, and triggers. These aren't strictly necessary, but if we have time they would be interesting and pertinent additions.
        Validation extensions will provide a way to validate incoming data from clients and reject attempts to synchronize invalid data.
        Conflict resolution extensions allow for a flexible and domain specific approach to resolving attempts to synchronize data that causes a conflict where the same data has been modified by two different clients.
        Trigger extensions will allow for arbitrary actions to be taken in the server when a client causes the data to change due to a sync.
    The Sync Service will have a network interface that communicates with the common client library (the ``Sync Client").
    The Sync Client will allow uniform data synchronization and access (via SQLite) across features in all clients.
    The diagram in Figure~\ref{fig:high-level} shows how these different components interact.

    % Diagram
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.95\linewidth]{./diagrams.pdf}
        \caption{A high level diagram of the proposed architecture. The ellipse shapes represent components that are specific to a particular domain application. The rounded rectangles represent components that are generic. \label{fig:high-level}}
    \end{figure}

    Ideally to demonstrate the versatility of our system, we will be able to write several (1-3) simple clients. % TODO: is this too optimistic/burdensome? should we say what they are?
    The focus of these clients will be demonstrating the system in different domains rather, so they won't have to be overcomplicated.
    Hopefully we can even demonstrate different platforms reusing the same service and common client code.

    It would also be interesting to perform load testing on the server to demonstrate the scalability of the system and understand how viable it would be in a real world deployment.

    % TODO: is there anything else interesting we could do within time?

    Our data model will be a relational one, where sets of tuples are stored in conceptual ``tables" that are synchronized between the server and client.
    Each tuple will be associated with a specific user, and those will be the only tuples that clients they have authorized will synchronize.
    Interestingly this does not necessitate using a relational database for the main Data Store, which only needs to retrieve tuples based on ownership.

    % TODO: Do we need to say how the time will be broken down?
    %   I'm thinking that it will be 5 hours of communication (1hr/week) plus probably 2-3 hours of design time,
    %   and then the implementation will probably be split roughly 50/50 client/server, and then hopefully debugging doesn't take too long...

    \section{What Will We Learn}

    % big system
    % "real" system
    % extensible system more complex than brittle single purpose system

\end{document}
